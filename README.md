
# NSW Working at Heights Safety Assistant  
A fully local, highâ€‘accuracy Retrievalâ€‘Augmented Generation (RAG) system designed to answer questions about **Working at Heights safety regulations in NSW, Australia**, built with productionâ€‘style architecture: ingestion, embeddings, vector search, local LLM inference, FastAPI backend, and Angular frontend.

This project demonstrates real machine learning engineering: RAG systems, embeddings, vector databases, LLM orchestration, API design, and UI integration.

---

## â­ Overview

The NSW Working at Heights Safety Assistant allows users to ask complex safety questions and receive clear, accurate answers sourced from real NSW government documents.

Everything runs **fully offline** using:
- Local embeddings  
- Local vector database (ChromaDB)  
- Local LLM via Ollama  
- Local backend (FastAPI)  
- Local frontend (Angular)

No cloud services, no API fees, no privacy risk.

---

## ğŸš€ Key Features

### ğŸ”¹ Highâ€‘Accuracy Retrieval  
- Uses document chunking + embedding similarity search  
- Powered by ChromaDB  
- Supports adding/removing documents dynamically  

### ğŸ”¹ Local LLM Reasoning  
- Default model: **llama3.1** via Ollama  
- Can swap to any offline model  
- No token limits or API cost  

### ğŸ”¹ Full API Backend  
- FastAPI async server  
- `/ask` endpoint that handles retrieval and generation  
- Clean, modular code  

### ğŸ”¹ Angular Frontend  
- Modern standalone Angular app  
- Real chat interface  
- Smooth typing animation  
- Clean, minimalist UI  

### ğŸ”¹ Realâ€‘World Use Case  
- Construction industry safety compliance is confusing  
- The assistant simplifies regulatory text into usable guidance  
- Perfect for workers, supervisors, and training environments  

---

## ğŸ§  Architecture

```
PDFs â†’ ingest.py â†’ ChromaDB â†’ query.py â†’ FastAPI â†’ Angular UI â†’ User
                     â†‘                                     â†“
                 Embeddings                           Local LLM (Ollama)
```

- `ingest.py` converts PDF documents into chunks â†’ embeddings â†’ vector DB  
- `query.py` retrieves relevant chunks and generates final answers  
- `FastAPI` exposes a REST endpoint for the UI  
- `Angular` provides a user-friendly chat interface  

---

## ğŸ“ Folder Structure

```
project/
â”‚â”€â”€ data/pdf/                 # Source PDFs (NSW safety documents)
â”‚â”€â”€ embeddings/               # Local ChromaDB vector store
â”‚â”€â”€ rag_basic/
â”‚     â”œâ”€â”€ ingest.py           # Document loading + chunking + embeddings
â”‚     â”œâ”€â”€ query.py            # RAG retrieval + generation pipeline
â”‚     â””â”€â”€ __init__.py
â”‚â”€â”€ api/
â”‚     â””â”€â”€ main.py             # FastAPI backend with /ask endpoint
â”‚â”€â”€ safety-rag-ui/            # Angular frontend (chat UI)
â”‚â”€â”€ README.md
```

---

## ğŸ“Œ Setup Instructions

### 1. Install dependencies

#### Backend
```
pip install fastapi uvicorn chromadb sentence-transformers pypdf python-dotenv
```

#### Frontend
```
npm install
```

#### Ollama  
Download from: https://ollama.com

Then pull your model:
```
ollama pull llama3.1
```

---

## ğŸ”§ Step 1: Ingest Documents

Place NSW safety PDFs into:

```
data/pdf/
```

Run:
```
python -m rag_basic.ingest
```

This:
- Loads PDFs  
- Splits into chunks  
- Generates embeddings  
- Stores vectors in ChromaDB  

---

## ğŸ” Step 2: Test Retrieval + LLM Locally

```
python -m rag_basic.query
```

Example:
```
What is considered working at heights in NSW?
```

You will see:
- Retrieved chunks  
- Final answer  

---

## ğŸŒ Step 3: Run FastAPI Backend

```
uvicorn api.main:app --reload
```

Endpoint:
```
POST http://127.0.0.1:8000/ask
```

---

## ğŸ’¬ Step 4: Run Angular Frontend

```
cd safety-rag-ui
ng serve
```

Visit:
```
http://localhost:4200
```

You now have a **fully working chat application**.

---

## ğŸ—ï¸ Technologies Used

### Machine Learning  
- Python  
- ChromaDB  
- SentenceTransformers  
- Ollama (local LLM)  
- RAG design (retrieve â†’ rank â†’ generate)  

### Backend  
- FastAPI  
- Pydantic  
- CORS middleware  
- Async REST endpoints  

### Frontend  
- Angular standalone components  
- Fetch-based communication  
- Modern chat interface  
- Responsive styling  






## ğŸ› ï¸ Future Enhancements

- Add a second safety domain (e.g., confined spaces, electrical hazards)
- Switch to hierarchical retrieval  
- Add rerankers (e.g., bge-reranker)  
- Multi-turn conversation with memory  
- Deployment to Docker  
- Optional cloud mode with OpenAI fallback  

---

## ğŸ‘¤ Author

**Shabab Saleheen**  
Machine Learning Engineer â€¢ Full-Stack Developer  
Sydney, Australia  

---

